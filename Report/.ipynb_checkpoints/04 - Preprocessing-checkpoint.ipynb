{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "413d63e3",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "## Part 1: Basic Preprocessing\n",
    "Here we will be looking at reducing the plots down into a workable list of tokens. The idea here is to simplify as much as possible whilst still keeping useful information about the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6516f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74c6b94",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Danie\\\\OneDrive\\\\Documents\\\\_Uni\\\\Maths\\\\Year 4\\\\Data Science Toolkit\\\\PreProcessedData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Loading in data frame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDanie\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDocuments\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m_Uni\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mMaths\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mYear 4\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mData Science Toolkit\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mPreProcessedData.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Creating list of film plots as a list of strings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plots\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mPlot\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Danie\\\\OneDrive\\\\Documents\\\\_Uni\\\\Maths\\\\Year 4\\\\Data Science Toolkit\\\\PreProcessedData.csv'"
     ]
    }
   ],
   "source": [
    "#Loading in data frame\n",
    "data=pd.read_csv('C:\\\\Users\\\\Danie\\\\OneDrive\\\\Documents\\\\_Uni\\\\Maths\\\\Year 4\\\\Data Science Toolkit\\\\PreProcessedData.csv')\n",
    "#Creating list of film plots as a list of strings\n",
    "plots=data.Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473a0e7",
   "metadata": {},
   "source": [
    "We begin by performing basic edits to the text. These include lowercasing all words for simplicity and removing unnecesary punctuation. One problem we found was hyphens in words were not counted as punctuation and left two words represented as one, such as 'ill-timed'. To fix this we split the word into by replacing the hyphen with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f41241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASIC TEXT EDITS\n",
    "import re\n",
    "#Switching out hyphens for spaces\n",
    "plots=[str(plots[num]).replace('-',' ') for num in range(len(plots))]\n",
    "# Removing punctuation\n",
    "plots=[re.sub(r'[^\\w\\s]','', str(plots[num])) for num in range(len(plots))]\n",
    "# Lowercasing the words\n",
    "plots=[str(plots[num]).lower() for num in range(len(plots))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d007ddf0",
   "metadata": {},
   "source": [
    "Next we remove all words that we know will be irrelevant in the final model. These include prepositions such as 'above, behind, with', predeterminers such as 'both, many', and pronouns etc. Fortunately in natural language processing these are recognised as 'stopwords'. We can simply import a pre-made list of stopwords in the English language and remove them from our text. Once these are gone, we 'tokenize' our text: turning into a list of every word that appears. (Note: this automatically removes duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING STOP WORDS AND TOKENIZING\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Define english stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Define function for removing stop words + tokenizing\n",
    "def stopntokenize(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    new_text = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            new_text.append(w)\n",
    "    return(new_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187774b",
   "metadata": {},
   "source": [
    "Whilst removing stopwords does a lot to remove unnecessary words, we can also go one step further and remove anything that isn't an adjective, verb or noun. (We presume here that adverbs will be irrelevant to our final model as they rarely seem to provide a unique description of the text). To do this we use nltk's built in function for classifying each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORDREMOVER\n",
    "\n",
    "#Defining function for removing any word not a Noun, Adjective or Verb\n",
    "def wordremover(tokens):\n",
    "    wordtypes = nltk.pos_tag(tokens)\n",
    "    tokens_new=[]\n",
    "    for i in range(len(tokens)):\n",
    "        if wordtypes[i][1] in ['NN','NNP','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "            tokens_new.append(tokens[i])\n",
    "    return(tokens_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe28aa6",
   "metadata": {},
   "source": [
    "Now that we have only our most important words, the next step is to reduce them down into their simplest form, or their 'stem'. For instance, we would like the words 'drink','drinks', and 'drinking' to all be shortened to 'drink' for simplicity. Whilst separate functions exist for stemming and lemmatizing separately such as PorterStemmer in nltk, we found that these tended to be quite bad at over or understemming for anything other than verbs. Wordnet's built in function 'morphy' seemed to work the best in reducing any word down without many errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEAMMATIZING AND STEMMING\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#Defining function to reduce word down it's simplest form\n",
    "def lemmanstem(tokens):\n",
    "    new_tokens=[]\n",
    "    for i in range(len(tokens)):\n",
    "        if wordnet.morphy(tokens[i])==None:\n",
    "            new_tokens.append(tokens[i])\n",
    "        else:\n",
    "            new_tokens.append(wordnet.morphy(tokens[i]))\n",
    "    return(new_tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd49548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NONE REMOVER\n",
    "\n",
    "#Defining function to remove Nones\n",
    "def noneremover(tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i]==None:\n",
    "            tokens.remove(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a579ddd",
   "metadata": {},
   "source": [
    "Now that we have all our functions defined, we can combine them all into one master function, which we use to turn the text as a string into a list of simplified tokens to use in our topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad99c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STRING TO TOKENS\n",
    "def tokenizer(text):\n",
    "    token=stopntokenize(text)\n",
    "    token=wordremover(token)\n",
    "    token=lemmanstem(token)\n",
    "    token=list(dict.fromkeys(token))\n",
    "    noneremover(token)\n",
    "    return(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85cf340",
   "metadata": {},
   "source": [
    "To check how our tokenizer is working, let's try a short plot as an example, the plot to 'My Best Friend's Wedding'. On inspection this seems to be working well. All the stop words are gone, all the verbs have been reduced to their most basic form, and all the nouns are in a singular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=plots[4]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4591a",
   "metadata": {},
   "source": [
    "This seems to work well in reducing the text to a few words, but let's try to see how this working on average across the whole dataset. On average, we can see that our tokenizer reduces the amount of words in the plot by about 93%, which is a massive reduction. Objectively, we have that each plot is reduced to about 40 words. This seems like a workable amount whilst still having the potential to include all relevant information about the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebe1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD REDUCTION \n",
    "\n",
    "#Calculating percentage reduction of words for plots and tokenized plots\n",
    "x=[]\n",
    "for i in range(4000):\n",
    "    x.append(len(tokenizer(plots[i]))/len(plots[i]))\n",
    "\n",
    "sum(x)/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8fd33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA SIZE\n",
    "\n",
    "#Calculating average number of words per tokenized plot\n",
    "#i.e. number of words remaining after text preprocessing\n",
    "x=[]\n",
    "for i in range(4000):\n",
    "    x.append(len(tokenizer(plots[i])))\n",
    "\n",
    "sum(x)/4000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11684271",
   "metadata": {},
   "source": [
    "Now that we are confident in the ability of our text preprocessing, we can apply our tokenizer to every plot in our dataset. This gives us the dataset that we will be using for the topic models in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING DATASET OF TOKENIZED PLOTS\n",
    "token_plots=[]\n",
    "for i in range(len(plots)):\n",
    "    token_plots.append(tokenizer(plots[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cfbb8",
   "metadata": {},
   "source": [
    "To use this list of lists in other python files, we convert each list into a string, save it to a .csv file, then this can be converted back into a list using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b41e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringed_plots=[str(token_plots[num]) for num in range(len(token_plots))]\n",
    "df=pd.DataFrame(stringed_plots)\n",
    "df.to_csv('stringed_plots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02621aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE FOR CONVERTING DATAFRAME OF STRINGS INTO LIST OF LISTS OF TOKENS\n",
    "#stringed_data=pd.read_csv('stringed_plots.csv')\n",
    "#tokenized_plots=[]\n",
    "#for i in range(0,len(stringed_data)):\n",
    "#    data=np.array(stringed_data.iloc[i])\n",
    "#    text=data[1]\n",
    "#    text=text.replace(',','')\n",
    "#    text=text.replace('[','')\n",
    "#    text=text.replace(']','')\n",
    "#    text=text.replace(\"'\",'')\n",
    "#    tokens=word_tokenize(text)\n",
    "#    tokenized_plots.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18820e2d",
   "metadata": {},
   "source": [
    "##Part 2: Synonym extension\n",
    "\n",
    "This is a good point to stop with the preprocessing, although we can potentially go one step further and look at the effect of synonyms. One potential issue when creating the topic model is removing words which do not appear too often in the dictionary. The problem here is that this potentially removes rarer synonyms of a more common word, whilst both could potentially be significant to the topic. For instance, 'conflict, war, battle, skirmish' all mean pretty much the same thing, and we would expect them to be significant to our model if they appear in a plot. However out of these 'war' is probably going to appear a lot more than the other three, so it could be that the other three are unjustly removed. It would be beneficial to us if we could reduce 'conflict, battle, skirmish' to 'war', which would essentially save these words from being removed. \n",
    "\n",
    "Hence we need a function that takes a word, checks all its potential synonyms, and then picks the one most likely to be kept after removing extremes in our dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f50c2",
   "metadata": {},
   "source": [
    "To start, we need to create a ranking of all words in the tokenized plots based off how much they are repeated throughout our dataset. This will come in useful when defining our synonym function. We start by creating a dictionary of all words, and then a dictionary with extremes removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eba514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING DICTIONARY\n",
    "import gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(token_plots)\n",
    "dictionary_trimmed = gensim.corpora.Dictionary(token_plots)\n",
    "\n",
    "#We remove words which come up too often or not often enough\n",
    "dictionary_trimmed.filter_extremes(no_below=25, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9195efa",
   "metadata": {},
   "source": [
    "We sort this dictionary by its counts, and translate this into a list of ranked words. We can see here the top ten most popular words across all plots, with the top three being 'life, find, new'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4871ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINDING TOP WORDS\n",
    "\n",
    "#Creating dictionary of wordcounts\n",
    "new_dict=dictionary.cfs\n",
    "new_dict_trimmed=dictionary_trimmed.cfs\n",
    "#Sorting by wordcount\n",
    "new_dict2 = sorted(new_dict.items(), key=lambda x:x[1],reverse=True)\n",
    "new_dict2_trimmed = sorted(new_dict_trimmed.items(), key=lambda x:x[1],reverse=True)\n",
    "#Translating word index to word\n",
    "ranked_words=[]\n",
    "for i in range(len(new_dict2)):\n",
    "    ranked_words.append(dictionary[new_dict2[i][0]])\n",
    "ranked_words_trimmed=[]\n",
    "for i in range(len(new_dict2_trimmed)):\n",
    "    ranked_words_trimmed.append(dictionary_trimmed[new_dict2_trimmed[i][0]])\n",
    "#Top 10 words\n",
    "ranked_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab771b1",
   "metadata": {},
   "source": [
    "One issue to address first is wordnet has multiple definitions for each word, so we need to make sure we are using the most common one. nltk already has the built in function pos_tag, although this is not always reliable. Often this decides a word is a noun by default if it is possible for it to be one, even if it is rarely used as such. For instance, most people would agree the most normal usage of 'jump' is as a verb, but since it can be used as a noun, pos_tag says it is one. This leads us to try and define our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(['jump'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce047c5",
   "metadata": {},
   "source": [
    "There is no guaranteed way to say what type of word a word is in our list of tokens without referring to its context within the original text. Instead we use the word's most common defintion. One way of estimating this is to count up every possible definition of a word, then pick the word type which is referred to the most. For instance, for the word 'jump', there are many slightly different definitions of the word. However by looking at all possibilities, we can see that it by far mosty used as a verb, so we would assign it as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet.synsets('jump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03196d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING OUR OWN POS_TAG\n",
    "def poz_tag(word):\n",
    "    #Count up the amounts of each type of word definition\n",
    "    #Wordnet uses 'n' for nouns, 'a' for adjectives, 'r' for adverbs, and 'v' and 's' for verbs\n",
    "    verbcount=len(wordnet.synsets(word,'v'))\n",
    "    verbcount2=len(wordnet.synsets(word,'s'))\n",
    "    nouncount=len(wordnet.synsets(word,'n'))\n",
    "    adjcount=len(wordnet.synsets(word,'a'))\n",
    "    adverbcount=len(wordnet.synsets(word,'r'))\n",
    "    #Find maximum count\n",
    "    wordtype=max(verbcount,nouncount,adjcount,verbcount2,adverbcount)\n",
    "    #Return the most basic definition of the most popular type of word\n",
    "    #(If the maximum is shared by multiple, we return the type of word in this order, arbitrarily)\n",
    "    if wordtype==nouncount:\n",
    "        return('n')\n",
    "    elif wordtype==verbcount:\n",
    "        return('v')\n",
    "    elif wordtype==adjcount:\n",
    "        return('a')\n",
    "    elif wordtype==verbcount2:\n",
    "        return('s')\n",
    "    elif wordtype==adverbcount:\n",
    "        return('r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed30b5a",
   "metadata": {},
   "source": [
    "Now we have dealt with this problem, we are ready to define our word to synonym transformer. We begin by creating a list of synonyms for the word which we can do using in-built functions in wordnet. We make sure however we restrict the search to only synonyms of the type of word poz_tag gives us. Once we have a selection of synonyms to choose from, we select the 'best' one. As a metric for this, we first look at the synonyms that are in our ranked words, i.e. the ones that would be saved from potential trimming of the data. Out of these we choose the most 'significant', using the index of the ranked words as a metric. (Note that it is possible that multiple words might be mapped to the same synonym, so we remove duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining function to create a list of synonyms for a word\n",
    "def synonyms(word):\n",
    "    list_of_synonyms= []\n",
    "    \n",
    "    #For each synset of the word, but only those under our poz_tag\n",
    "    for syn in wordnet.synsets(word,poz_tag(word)):\n",
    "        for l in syn.lemmas():\n",
    "            list_of_synonyms.append(l.name())\n",
    "    list_of_synonyms=list(dict.fromkeys(list_of_synonyms))\n",
    "\n",
    "\n",
    "    return(list_of_synonyms)\n",
    "\n",
    "#Defining function to map a word to its best synonym\n",
    "def wordtosynonym(word):\n",
    "    #Creating the word's list of synonyms\n",
    "    word_synonyms=synonyms(word)\n",
    "    master_word=word\n",
    "    for i in word_synonyms:\n",
    "        #Calculate the ranking of the word based off its count in the dictionary\n",
    "        sig1 = ranked_words.index(word)\n",
    "        #Only turn the word into a synonym if the synonym is in the trimmed dictionary\n",
    "        if i in ranked_words_trimmed:      \n",
    "            sig2 = ranked_words.index(i)\n",
    "            #We set this synonym as the 'best synonym' if its ranking is lower than the others\n",
    "            if sig2<sig1:\n",
    "                master_word=i\n",
    "    return(master_word)\n",
    "\n",
    "#Function applying wordtosynonym to every word in a list of tokens\n",
    "def tokenstosynonyms(tokens,example):\n",
    "    new_tokens=[]\n",
    "    for word in tokens:\n",
    "        new_word=wordtosynonym(word)\n",
    "        if example:\n",
    "            if new_word==word:\n",
    "                print(Fore.BLUE+word,Fore.BLUE+new_word)\n",
    "            else:\n",
    "                print(Fore.BLUE+word,Fore.RED+new_word)\n",
    "        new_tokens.append(new_word)\n",
    "    new_tokens=list(dict.fromkeys(new_tokens))\n",
    "    return(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65513bfa",
   "metadata": {},
   "source": [
    "We can see how this works using the plot of the film 'Up at the Villa' as an example. Here for ever word, we have the original word on the left and its 'optimal synonym' on the right. If different, they are displayed in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f78fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenstosynonyms(token_plots[50],True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35fae86",
   "metadata": {},
   "source": [
    "To see the effect of this, let us consider what happens when we remove the words removed in our trimmed dictionary to the tokenized plots. This is something we expect to happen in the topic model sections later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonymed_plots=[]\n",
    "for i in range(1000):\n",
    "    synonymed_plots.append(tokenstosynonyms(token_plots[i],False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54cea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function that removes the words we defined as 'extreme'\n",
    "def wordtrimmer(tokens,rank):\n",
    "    new_tokens=[]\n",
    "    for word in tokens:\n",
    "        if word in rank:\n",
    "            new_tokens.append(word)\n",
    "    return(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366452c",
   "metadata": {},
   "source": [
    "Here we can the average number of words after trimming is slightly higher for the synonymed plots, meaning we have been able to keep extra words whilst still retaining effectively the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([len(wordtrimmer(token,ranked_words_trimmed)) for token in synonymed_plots])/1000)\n",
    "print(sum([len(wordtrimmer(token_plots[num],ranked_words_trimmed)) for num in range(1000)])/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa621e2",
   "metadata": {},
   "source": [
    "As a specific example, let's consider the plot of 'Wild Child'. Here we have the tokenized plots and the tokenized plots after having their extreme tokens removed, with the original in red and the synonymed version in blue. Firstly, the synonymed version has 2 less words than the original, most likely from removing 2 synonyms of each other within the plot. However, after trimming, the synonymed version actually has seven more words. It is difficult to see with the words written in a slightly different order, but the synonymed version has changed:\n",
    "\n",
    "'regime' to 'government'\n",
    "\n",
    "'dismiss' to 'fire'\n",
    "\n",
    "'appeal' to 'attract'\n",
    "\n",
    "all of which were originally removed but have now been saved from removal in the synonymed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M=401\n",
    "#Original tokenized list\n",
    "print(Fore.RED+str(token_plots[M]))\n",
    "#Number of words in original list\n",
    "print(len(token_plots[M]))\n",
    "#Synonymed list\n",
    "print(Fore.BLUE+str(synonymed_plots[M]))\n",
    "#Number of words in synonymed list\n",
    "print(len(synonymed_plots[M]))\n",
    "#Original list after extreme words removed\n",
    "print(Fore.RED+str(wordtrimmer(token_plots[M],ranked_words_trimmed)))\n",
    "#Number of words remaining after trimming\n",
    "print(len(wordtrimmer(token_plots[M],ranked_words_trimmed)))\n",
    "#Synonymed list after extreme words removed\n",
    "print(Fore.BLUE+str(wordtrimmer(synonymed_plots[M],ranked_words_trimmed)))\n",
    "#Number of words remaining after trimming\n",
    "print(len(wordtrimmer(synonymed_plots[M],ranked_words_trimmed)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
