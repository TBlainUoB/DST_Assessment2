{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "\n",
    "As seen in the previous section, we found the mean absolute error for each of our four models: Our baseline prediction which just returned the average viewer rating, our two LDA models: one using basic preprocessing and one using improved preprocessing, and our pretrained Hugging Face transformer model. Unfortunately we found the HDP model not to work so we will omit it from our evaluation.\n",
    "\n",
    "The mean absolute error works as a simple metric for this kind of prediction model, where we simply calculate the average absolute difference between each prediction and its true value $|x_i-y_i|$. This is commonly used in prediction models elsewhere and is very similar to the also commonly used mean squared error. Our models ranked by order of MAE, with the best having lowest MAE, are:\n",
    "\n",
    " - 1. LDA model (Synonyms): 0.716 MAE\n",
    " \n",
    " - 2. LDA model (Standard): 0.725 MAE\n",
    " \n",
    " - 3. Hugging Face transformer: 0.760 MAE\n",
    " \n",
    " - 4. Baseline: 0.826 MAE\n",
    " \n",
    "\n",
    "Unsurprisingly the baseline performed the worst, which is a good sign for our three models. The pretrained model coming in at 3rd also makes sense as it was not fine tuned to this specific problem in the way that the LDA and HDP models were. However its score is still impressive considering the fact is was able to replicate a lot of our own project chapters in only a few lines of code. Interestly, the synonym model did slightly outperform the standard LDA model, which implies that a good way to improve performance in the LDA models is to fine tune the preprocessing of the input data. \n",
    "\n",
    "It is not entirely clear what advantage of the synonym model was the cause of the improved performance. As mentioned during 04 - Preprocessing, the motivation behind it was to 'save' words from being removed, and to increase the number of more significant words. The better performance gives evidence to this having an effect; it could be that by having more frequent words kept in the plots, a plot was more likely to have a word appear in a topic and have that information available for the predictor model.\n",
    "\n",
    "## Effect of the Topic Model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
