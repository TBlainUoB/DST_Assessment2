{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "\n",
    "As seen in the previous section, we found the mean absolute error for each of our four models: Our baseline prediction which just returned the average viewer rating, our two LDA models: one using basic preprocessing and one using improved preprocessing, and our pretrained Hugging Face transformer model. Unfortunately we found the HDP model not to work so we will omit it from our evaluation.\n",
    "\n",
    "The mean absolute error works as a simple metric for this kind of prediction model, where we simply calculate the average absolute difference between each prediction and its true value $|x_i-y_i|$. This is commonly used in prediction models elsewhere and is very similar to the also commonly used mean squared error. Our models ranked by order of MAE, with the best having the lowest MAE, are:\n",
    "\n",
    " - 1. LDA model (Synonyms): 0.716 MAE\n",
    " \n",
    " - 2. LDA model (Standard): 0.715 MAE\n",
    " \n",
    " - 3. Hugging Face transformer: 0.760 MAE\n",
    " \n",
    " - 4. Baseline: 0.826 MAE\n",
    "\n",
    " - 5. Baseline boosting algorithm with text features removed: 0.695\n",
    " \n",
    "\n",
    "Unsurprisingly the baseline performed the worst, which is a good sign for our three models. The pretrained model coming in at 4th also makes sense as it was a brute force embedding of the text, with the hopes of finding meaningful embeddings. For this task, a more fine tuned approach was needed to better understand what information the plots give us, such as topic modelling. Interestly, the synonym model did slightly outperform the standard LDA model, which implies that a good way to improve performance in the LDA models is to fine tune the preprocessing of the input data.\n",
    "\n",
    "It is not entirely clear what advantage of the synonym model was the cause of the improved performance. As mentioned during 04 - Preprocessing, the motivation behind it was to 'save' words from being removed, and to increase the number of more significant words. The better performance gives evidence to this having an effect; it could be that by having more frequent words kept in the plots, a plot was more likely to have a word appear in a topic and have that information available for the predictor model.\n",
    "\n",
    "## Effect of the Topic Model\n",
    "\n",
    "It is regretful to see that the boosting algorithm without any text features was the best model we trained. It seems as if the added complexity from the text data only harmed our models predictions. It is unlikely that this is due to overfitting, since the validation scores were also consistent with this result. I would also not conclude that it was down to bad text data, The plot data was complete and of a good length for training.\n",
    "\n",
    "To gain more insight into this, we can produce a random forest to measure feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. t6 (0.069979)\n",
      "2. t1 (0.061592)\n",
      "3. t8 (0.061389)\n",
      "4. t7 (0.060017)\n",
      "5. t5 (0.059447)\n",
      "6. t4 (0.059300)\n",
      "7. Horror (0.058862)\n",
      "8. t3 (0.057350)\n",
      "9. t9 (0.055264)\n",
      "10. t2 (0.055160)\n",
      "11. Year (0.048794)\n",
      "12. Drama (0.048281)\n",
      "13. Documentary (0.039883)\n",
      "14. Biography (0.014063)\n",
      "15. Animation (0.010882)\n",
      "16. Comedy (0.009644)\n",
      "17. Action (0.008749)\n",
      "18. Bruce Willis (0.008608)\n",
      "19. Adventure (0.007874)\n",
      "20. Miley Cyrus (0.007002)\n",
      "21. Thriller (0.005326)\n",
      "22. Crime (0.004219)\n",
      "23. Mystery (0.004146)\n",
      "24. Family (0.003596)\n",
      "25. Sport (0.003558)\n",
      "26. Fantasy (0.003184)\n",
      "27. Sci-Fi (0.003137)\n",
      "28. Romance (0.002940)\n",
      "29. Patrick Muldoon (0.002745)\n",
      "30. Tyler Perry (0.002546)\n",
      "31. Danny Dyer (0.002489)\n",
      "32. Short (0.002375)\n",
      "33. Jaime Pressly (0.002259)\n",
      "34. Music (0.002235)\n",
      "35. War (0.002094)\n",
      "36. Robert Downey Jr. (0.001957)\n",
      "37. Jon Voight (0.001843)\n",
      "38. Larry the Cable Guy (0.001767)\n",
      "39. Nick Cannon (0.001721)\n",
      "40. Christian Bale (0.001702)\n",
      "41. James Corden (0.001628)\n",
      "42. Suki Waterhouse (0.001477)\n",
      "43. Ethan Hawke (0.001454)\n",
      "44. Zoe Saldana (0.001413)\n",
      "45. Chow Yun-Fat (0.001400)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.read_csv(\"Data/PreProcessedData.csv\")\n",
    "df_no_plot = df1.drop('Plot', axis = 1)\n",
    "df2 = pd.read_csv(\"Data/LDA_topics_synonym.csv\")\n",
    "df_LDATOPICS_synonym = pd.concat([df_no_plot, df2], axis=1)\n",
    "df_LDATOPICS_synonym = df_LDATOPICS_synonym.drop('Title', axis = 1)\n",
    "df_LDATOPICS_synonym = df_LDATOPICS_synonym.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "y = df_LDATOPICS_synonym['IMDbRating']\n",
    "X = df_LDATOPICS_synonym.drop(['IMDbRating'], axis=1)\n",
    "\n",
    "# Create a random forest regressor with 100 trees\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Print the feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(45):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also explore removing the actor columns from our dataset and retraining all models.\n",
    "All models now perform significantly worse, indicating that even though the actor columns added a lot of complexity to our model, they work as good predictors.\n",
    "\n",
    " - 1. LDA model (Synonyms): 0.770 MAE\n",
    "\n",
    " - 2. LDA model (Standard): 0.774 MAE\n",
    "\n",
    " - 3. Baseline: 0.826 MAE\n",
    "\n",
    " - 4. Baseline boosting algorithm with text features removed: 0.759"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A possible explanation for why we are not getting the results we expect"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have concluded that the additional text features are not overfitting, however, they may be introducing noise into the model which weakens the overall predictive performance. It could be that despite our expectations, movie plots are not a very good predictor and that anyone can write a good movie plot."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
