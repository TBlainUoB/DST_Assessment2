{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380a1ffc",
   "metadata": {},
   "source": [
    "#Text Preprocessing\n",
    "\n",
    "Here we will be looking at reducing the plots down into a workable list of tokens. The idea here is to simplify as much as possible whilst still keeping useful information about the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6516f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74c6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in data frame\n",
    "data=pd.read_csv('C:\\\\Users\\\\Danie\\\\OneDrive\\\\Documents\\\\_Uni\\\\Maths\\\\Year 4\\\\Data Science Toolkit\\\\PreProcessedData.csv')\n",
    "#Creating list of film plots as a list of strings\n",
    "plots=data.Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870340b",
   "metadata": {},
   "source": [
    "We begin by performing basic edits to the text. These include lowercasing all words for simplicity and removing unnecesary punctuation. One problem we found was hyphens in words were not counted as punctuation and left two words represented as one, such as 'ill-timed'. To fix this we split the word into by replacing the hyphen with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f41241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASIC TEXT EDITS\n",
    "import re\n",
    "#Switching out hyphens for spaces\n",
    "plots=[str(plots[num]).replace('-',' ') for num in range(len(plots))]\n",
    "# Removing punctuation\n",
    "plots=[re.sub(r'[^\\w\\s]','', str(plots[num])) for num in range(len(plots))]\n",
    "# Lowercasing the words\n",
    "plots=[str(plots[num]).lower() for num in range(len(plots))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10c33e",
   "metadata": {},
   "source": [
    "Next we remove all words that we know will be irrelevant in the final model. These include prepositions such as 'above, behind, with', predeterminers such as 'both, many', and pronouns etc. Fortunately in natural language processing these are recognised as 'stopwords'. We can simply import a pre-made list of stopwords in the English language and remove them from our text. Once these are gone, we 'tokenize' our text: turning into a list of every word that appears. (Note: this automatically removes duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f63024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING STOP WORDS AND TOKENIZING\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Define english stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Define function for removing stop words + tokenizing\n",
    "def stopntokenize(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    new_text = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            new_text.append(w)\n",
    "    return(new_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34cb46",
   "metadata": {},
   "source": [
    "Whilst removing stopwords does a lot to remove unnecessary words, we can also go one step further and remove anything that isn't an adjective, verb or noun. (We presume here that adverbs will be irrelevant to our final model as they rarely seem to provide a unique description of the text). To do this we use nltk's built in function for classifying each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9f5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORDREMOVER\n",
    "\n",
    "#Defining function for removing any word not a Noun, Adjective or Verb\n",
    "def wordremover(tokens):\n",
    "    wordtypes = nltk.pos_tag(tokens)\n",
    "    tokens_new=[]\n",
    "    for i in range(len(tokens)):\n",
    "        if wordtypes[i][1] in ['NN','NNP','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "            tokens_new.append(tokens[i])\n",
    "    return(tokens_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168b888",
   "metadata": {},
   "source": [
    "Now that we have only our most important words, the next step is to reduce them down into their simplest form, or their 'stem'. For instance, we would like the words 'drink','drinks', and 'drinking' to all be shortened to 'drink' for simplicity. Whilst separate functions exist for stemming and lemmatizing separately such as PorterStemmer in nltk, we found that these tended to be quite bad at over or understemming for anything other than verbs. Wordnet's built in function 'morphy' seemed to work the best in reducing any word down without many errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e0796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEAMMATIZING AND STEMMING\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#Defining function to reduce word down it's simplest form\n",
    "def lemmanstem(tokens):\n",
    "    new_tokens=[]\n",
    "    for i in range(len(tokens)):\n",
    "        if wordnet.morphy(tokens[i])==None:\n",
    "            new_tokens.append(tokens[i])\n",
    "        else:\n",
    "            new_tokens.append(wordnet.morphy(tokens[i]))\n",
    "    return(new_tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dd49548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NONE REMOVER\n",
    "\n",
    "#Defining function to remove Nones\n",
    "def noneremover(tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i]==None:\n",
    "            tokens.remove(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c05dc",
   "metadata": {},
   "source": [
    "Now that we have all our functions defined, we can combine them all into one master function, which we use to turn the text as a string into a list of simplified tokens to use in our topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad99c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STRING TO TOKENS\n",
    "def tokenizer(text):\n",
    "    token=stopntokenize(text)\n",
    "    token=wordremover(token)\n",
    "    token=lemmanstem(token)\n",
    "    token=list(dict.fromkeys(token))\n",
    "    noneremover(token)\n",
    "    return(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c90396",
   "metadata": {},
   "source": [
    "To check how our tokenizer is working, let's try a short plot as an example, the plot to 'My Best Friend's Wedding'. On inspection this seems to be working well. All the stop words are gone, all the verbs have been reduced to their most basic form, and all the nouns are in a singular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "697f8322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a woman who by a promise made years earlier is supposed to marry her best friend in three weeks even though she doesnt want to when she finds out that hes marrying someone else she becomes jealous and tries to break off the wedding'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example=plots[4]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd2c01cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman',\n",
       " 'promise',\n",
       " 'make',\n",
       " 'suppose',\n",
       " 'marry',\n",
       " 'best',\n",
       " 'friend',\n",
       " 'doesnt',\n",
       " 'want',\n",
       " 'find',\n",
       " 'someone',\n",
       " 'become',\n",
       " 'jealous',\n",
       " 'break',\n",
       " 'wedding']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7005e",
   "metadata": {},
   "source": [
    "This seems to work well in reducing the text to a few words, but let's try to see how this working on average across the whole dataset. On average, we can see that our tokenizer reduces the amount of words in the plot by about 93%, which is a massive reduction. Objectively, we have that each plot is reduced to about 40 words. This seems like a workable amount whilst still having the potential to include all relevant information about the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ebe1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07198069794144195"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WORD REDUCTION \n",
    "\n",
    "#Calculating percentage reduction of words for plots and tokenized plots\n",
    "x=[]\n",
    "for i in range(4000):\n",
    "    x.append(len(tokenizer(plots[i]))/len(plots[i]))\n",
    "\n",
    "sum(x)/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c8fd33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.78775"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA SIZE\n",
    "\n",
    "#Calculating average number of words per tokenized plot\n",
    "#i.e. number of words remaining after text preprocessing\n",
    "x=[]\n",
    "for i in range(4000):\n",
    "    x.append(len(tokenizer(plots[i])))\n",
    "\n",
    "sum(x)/4000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e020575",
   "metadata": {},
   "source": [
    "Now that we are confident in the ability of our text preprocessing, we can apply our tokenizer to every plot in our dataset. This gives us the dataset that we will be using for the topic models in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "cd72c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING DATASET OF TOKENIZED PLOTS\n",
    "token_plots=[]\n",
    "for i in range(len(plots)):\n",
    "    token_plots.append(tokenizer(plots[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be011d89",
   "metadata": {},
   "source": [
    "To use this list of lists in other python files, we convert each list into a string, save it to a .csv file, then this can be converted back into a list using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ffafa8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringed_plots=[str(token_plots[num]) for num in range(len(token_plots))]\n",
    "df=pd.DataFrame(stringed_plots)\n",
    "df.to_csv('stringed_plots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "567cb366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE FOR CONVERTING DATAFRAME OF STRINGS INTO LIST OF LISTS OF TOKENS\n",
    "#stringed_data=pd.read_csv('stringed_plots.csv')\n",
    "#tokenized_plots=[]\n",
    "#for i in range(0,len(stringed_data)):\n",
    "#    data=np.array(stringed_data.iloc[i])\n",
    "#    text=data[1]\n",
    "#    text=text.replace(',','')\n",
    "#    text=text.replace('[','')\n",
    "#    text=text.replace(']','')\n",
    "#    text=text.replace(\"'\",'')\n",
    "#    tokens=word_tokenize(text)\n",
    "#    tokenized_plots.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ab592",
   "metadata": {},
   "source": [
    "This is a good point to stop with the preprocessing, although we can potentially go one step further and look at the effect of synonyms. One potential issue when creating the topic model is removing words which do not appear too often. The problem here is that this potentially removes rarer synonyms of a more common word, whilst both could potentially be significant to the topic. For instance, 'conflict, war, battle, skirmish' all mean pretty much the same thing, and we would expect them to be significant to our model if they appear in a plot. However out of these 'war' is probably going to appear a lot more than the other three, so it could be that the other three are unjustly removed. It would be beneficial to us if we could reduce 'conflict, battle, skirmish' to 'war', which would not only mean they are kept in the data, but also reduce the size of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893367d6",
   "metadata": {},
   "source": [
    "To start, we need to create a ranking of all words in the tokenized plots based off how much they are repeated throughout our dataset. This will come in useful when defining our synonym function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75eba514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24569"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CREATING DICTIONARY\n",
    "import gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(token_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27b5a0",
   "metadata": {},
   "source": [
    "We sort this dictionary by its counts, and translate this into a list of ranked words. We can see here the top ten most popular words across all plots, with the top three being 'life, find, new'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4871ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life', 'find', 'new', 'take', 'get', 'young', 'family', 'world', 'go', 'man']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FINDING TOP WORDS\n",
    "\n",
    "#Creating dictionary of wordcounts\n",
    "new_dict=dictionary.cfs\n",
    "#Sorting by wordcount\n",
    "new_dict2 = sorted(new_dict.items(), key=lambda x:x[1],reverse=True)\n",
    "#Translating word index to word\n",
    "ranked_words=[]\n",
    "for i in range(len(new_dict2)):\n",
    "    ranked_words.append(dictionary[new_dict2[i][0]])\n",
    "#Top 10 words\n",
    "ranked_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b56103",
   "metadata": {},
   "source": [
    "Now we are ready to define a synonym remover. The idea here is for a list of tokens, we search through each pair of tokens and use wordnet to find their 'similarity'. This gives us a numerical value of how close they are, and then by defining a threshold of similarity, we can define them as synonyms for each other. Once we have classfied them as synonyms, we keep only the more relevant one. To define their 'relevance', we use their index in our ranked words as a metric, keeping the one with the lower score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607bb5ff",
   "metadata": {},
   "source": [
    "One issue to address is wordnet has multiple definitions for each word, so we need to make sure we are using the most common one. For instance, consider the words 'drink' and 'swallow'. Their most common defintions are as verbs, and in this case, wordnet will find that they have a similarity of 0.33. However if our function thinks of them as nouns, i.e. drink as a synonym for a body of water and swallow as the bird, it will give a much lower similairty of 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14c31fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333 0.2\n"
     ]
    }
   ],
   "source": [
    "drink_verb=wordnet.synset('drink.v.01')\n",
    "swallow_verb=wordnet.synset('swallow.v.01')\n",
    "drink_noun=wordnet.synset('drink.n.01')\n",
    "swallow_noun=wordnet.synset('swallow.n.01')\n",
    "print(drink_verb.path_similarity(swallow_verb),drink_noun.path_similarity(swallow_noun))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3ea80",
   "metadata": {},
   "source": [
    "There is no guaranteed way to say what type of word a word is in our list of tokens without referring to its context within the original text. Instead we use the word's most common defintion. One way of estimating this is to count up every possible definition of a word, then pick the word type which is referred to the most. For instance, for the word 'jump', there are many slightly different definitions of the word. However by looking at all possibilities, we can see that it by far mosty used as a verb, so we would assign it as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "0884be6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('jump.n.01'),\n",
       " Synset('leap.n.02'),\n",
       " Synset('jump.n.03'),\n",
       " Synset('startle.n.01'),\n",
       " Synset('jump.n.05'),\n",
       " Synset('jump.n.06'),\n",
       " Synset('jump.v.01'),\n",
       " Synset('startle.v.02'),\n",
       " Synset('jump.v.03'),\n",
       " Synset('jump.v.04'),\n",
       " Synset('leap_out.v.01'),\n",
       " Synset('jump.v.06'),\n",
       " Synset('rise.v.11'),\n",
       " Synset('jump.v.08'),\n",
       " Synset('derail.v.02'),\n",
       " Synset('chute.v.01'),\n",
       " Synset('jump.v.11'),\n",
       " Synset('jumpstart.v.01'),\n",
       " Synset('jump.v.13'),\n",
       " Synset('leap.v.02'),\n",
       " Synset('alternate.v.01')]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('jump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f824cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINDING MOST COMMON DEFINITION\n",
    "def mostcommon(word):\n",
    "    #Count up the amounts of each type of word definition\n",
    "    verbcount=len(wordnet.synsets(word,'v'))\n",
    "    verbcount2=len(wordnet.synsets(word,'s'))\n",
    "    nouncount=len(wordnet.synsets(word,'n'))\n",
    "    adjcount=len(wordnet.synsets(word,'a'))\n",
    "    adverbcount=len(wordnet.synsets(word,'r'))\n",
    "    #Find maximum count\n",
    "    wordtype=max(verbcount,nouncount,adjcount,verbcount2,adverbcount)\n",
    "    #Return the most basic definition of the most popular type of word\n",
    "    #(If the maximum is shared by multiple, we return the type of word in this order, arbitrarily)\n",
    "    if wordtype==nouncount:\n",
    "        return(wordnet.synsets(word,'n')[0])\n",
    "    elif wordtype==verbcount:\n",
    "        return(wordnet.synsets(word,'v')[0])\n",
    "    elif wordtype==adjcount:\n",
    "        return(wordnet.synsets(word,'a')[0])\n",
    "    elif wordtype==verbcount2:\n",
    "        return(wordnet.synsets(word,'s')[0])\n",
    "    elif wordtype==adverbcount:\n",
    "        return(wordnet.synsets(word,'r')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b915d11",
   "metadata": {},
   "source": [
    "Now we have dealt with this problem, we are ready to define our synonym remover. Here we look at every unordered pair of tokens, find their most common definition, and use this to calculate the Wu-Palmer similarity. (We also check first to see if both words have any definitions at all, as the similarity will only work if it does. e.g. Hogwarts and Castle will not work.) After manually tweaking the threshold to see what gives the best result, we find that 0.65 works quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "98f8cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING SYNONYM REMOVER\n",
    "from colorama import Fore\n",
    "\n",
    "def synonymremover(tokens,text):\n",
    "    #Defining list of unneccesary synonyms\n",
    "    synonyms=[]\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            if i<j:      \n",
    "                word1=tokens[i]\n",
    "                word2=tokens[j]\n",
    "                #Get list of what the word could mean\n",
    "                list_of_words1=wordnet.synsets(word1)\n",
    "                list_of_words2=wordnet.synsets(word2)\n",
    "                #Check that word has a possible definition\n",
    "                if not (list_of_words1==[] or list_of_words2==[]):\n",
    "                    #Define words as the most common \n",
    "                    word_1=mostcommon(word1)\n",
    "                    word_2=mostcommon(word2)\n",
    "                    #Caculate similarity\n",
    "                    similarity=(word_1).wup_similarity(word_2)\n",
    "                    #If they are close enough, add less significant word to synonyms\n",
    "                    if similarity>0.65 and similarity !=1:\n",
    "                        #Calculate significance by getting their rank from the ranked words\n",
    "                        sig1 = ranked_words.index(word1)\n",
    "                        sig2 = ranked_words.index(word2)\n",
    "                        #Add whichever word is lower down in the ranking to synonyms\n",
    "                        if sig1<sig2:\n",
    "                            if text:\n",
    "                                print(Fore.BLUE+word1,Fore.RED+word2)\n",
    "                            synonyms.append(tokens[j])\n",
    "                        else:\n",
    "                            if text:\n",
    "                                print(Fore.BLUE+word2,Fore.RED+word1)\n",
    "                            synonyms.append(tokens[i])\n",
    "    #Remove all unneccesary synonyms\n",
    "    for word in tokens:\n",
    "        if word in synonyms:\n",
    "            tokens.remove(word)\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335cfcb4",
   "metadata": {},
   "source": [
    "We can see how this works using the plot of the film 'Millions' as an example. Here if two words are declared as synonyms by our function, it prints the more relevant one we will keep in blue and the less relevant one we will omit in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "b0251f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mmoney \u001b[31mcurrency\n",
      "\u001b[34mpound \u001b[31meuro\n",
      "\u001b[34mload \u001b[31mplayhouse\n",
      "\u001b[34mfilm \u001b[31mload\n",
      "\u001b[34mfilm \u001b[31mplayhouse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['uk',\n",
       " 'switch',\n",
       " 'pound',\n",
       " 'giving',\n",
       " 'gang',\n",
       " 'chance',\n",
       " 'rob',\n",
       " 'secure',\n",
       " 'train',\n",
       " 'money',\n",
       " 'way',\n",
       " 'incineration',\n",
       " 'robbery',\n",
       " 'big',\n",
       " 'falls',\n",
       " 'sky',\n",
       " 'year',\n",
       " 'old',\n",
       " 'given',\n",
       " 'talking',\n",
       " 'boy',\n",
       " 'start',\n",
       " 'seeing',\n",
       " 'world',\n",
       " 'make',\n",
       " 'human',\n",
       " 'soul',\n",
       " 'come',\n",
       " 'forefront',\n",
       " 'film']"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonymremover(tokenizer(plots[200]),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "7fa8f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATING TOKENIZED PLOTS\n",
    "#Takes quite a while atm lol\n",
    "#200\n",
    "\n",
    "#for i in range(100):\n",
    " #   token_plots[i]=synonymremover(token_plots[i],False)\n",
    "synonymremover(tokenizer(plots[200]),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "e1098946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('hogwarts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
