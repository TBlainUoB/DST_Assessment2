{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# XGBoost Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We choose to implement an XGBoost model since it has been a high performance model in our previous data science experience. For our rating prediction task we want the model to have the ability to handle complex data as well as being fast and efficient.\n",
    "\n",
    "For our loss function we choose to use MeanAbsoluteError(MAE). This is a standard loss function for this task and does not penalise outlier data as strongly as MeanSquaredError(MSE). This provides more interpretability to our results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin by concatenating our LDA models with the output dataset from 02, These models have not used the 'Title' column in our dataset since the short titles of films felt as if they would be weak predictors, except in the rare case of sequels. The original 'Plot' and 'Title' column are then dropped so the data is now in a format which can be input for the XGBoost model.\n",
    "\n",
    "For an XGBoost model, we do not need to normalize our data since the model is gradient boosted decision trees.\n",
    "\n",
    "We also should create a baseline method which we will be able to compare against to evaluate model performance.\n",
    "For a simple baseline, we choose to take the mean 'IMDbRating' from the training set.\n",
    "For a more advanced baseline to test how the text processing performs, we can run an XGBoost model which does not use any of the 'Title' or 'Plot' data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Data/PreProcessedData.csv\")\n",
    "df_no_plot = df1.drop('Plot', axis = 1)\n",
    "df2 = pd.read_csv(\"Data/LDA_topics.csv\") #UPDATE THIS TO BE WHATEVER DATA WE GIVE IT\n",
    "df_LDATOPICS = pd.concat([df_no_plot, df2], axis=1)\n",
    "df_LDATOPICS = df_LDATOPICS.drop('Title', axis = 1)\n",
    "df_LDATOPICS = df_LDATOPICS.drop('Unnamed: 0', axis = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      IMDbRating  Year  Action  Adult  Adventure  Animation  Biography  \\\n",
      "0            7.1  2000       0      0          0          0          0   \n",
      "1            4.1  2000       0      0          0          0          0   \n",
      "2            6.6  2000       0      0          1          0          0   \n",
      "3            5.6  2000       0      0          0          0          0   \n",
      "4            7.7  2000       0      0          0          0          0   \n",
      "...          ...   ...     ...    ...        ...        ...        ...   \n",
      "6509         4.0  2022       0      0          0          0          0   \n",
      "6510         7.9  2022       0      0          1          1          0   \n",
      "6511         7.6  2022       0      0          0          0          0   \n",
      "6512         6.9  2022       0      0          0          0          1   \n",
      "6513         6.7  2022       0      0          0          0          0   \n",
      "\n",
      "      Comedy  Crime  Documentary  ...  ZoÃ« Kravitz        t1        t2  \\\n",
      "0          0      0            0  ...            0  0.077924  0.101854   \n",
      "1          0      0            0  ...            0  0.163846  0.062531   \n",
      "2          0      0            0  ...            0  0.038255  0.040860   \n",
      "3          1      0            0  ...            0  0.328731  0.054868   \n",
      "4          0      0            0  ...            0  0.181095  0.203146   \n",
      "...      ...    ...          ...  ...          ...       ...       ...   \n",
      "6509       0      0            0  ...            0  0.121991  0.055460   \n",
      "6510       1      0            0  ...            0  0.073794  0.094170   \n",
      "6511       1      0            0  ...            0  0.066596  0.065707   \n",
      "6512       0      0            0  ...            0  0.064920  0.071956   \n",
      "6513       0      1            0  ...            0  0.061650  0.098114   \n",
      "\n",
      "            t3        t4        t5        t6        t7        t8        t9  \n",
      "0     0.047969  0.065177  0.028895  0.062591  0.120463  0.141317  0.353810  \n",
      "1     0.083822  0.057579  0.354931  0.054491  0.058942  0.062341  0.101517  \n",
      "2     0.123012  0.157540  0.075882  0.097192  0.192975  0.048553  0.225731  \n",
      "3     0.063526  0.108234  0.038530  0.117623  0.050715  0.098630  0.139144  \n",
      "4     0.101026  0.217619  0.041514  0.079515  0.037983  0.105121  0.032981  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "6509  0.069137  0.050924  0.118014  0.050923  0.316973  0.163412  0.053168  \n",
      "6510  0.239153  0.060654  0.054820  0.061588  0.130182  0.058019  0.227619  \n",
      "6511  0.054932  0.225038  0.101555  0.283400  0.079277  0.061585  0.061909  \n",
      "6512  0.069863  0.356622  0.095750  0.068859  0.110435  0.059482  0.102113  \n",
      "6513  0.258785  0.063227  0.059371  0.064161  0.103529  0.068393  0.222770  \n",
      "\n",
      "[6514 rows x 965 columns]\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"Data/LDA_topics_synonym.csv\")\n",
    "df_LDATOPICS_synonym = pd.concat([df_no_plot, df2], axis=1)\n",
    "df_LDATOPICS_synonym = df_LDATOPICS_synonym.drop('Title', axis = 1)\n",
    "df_LDATOPICS_synonym = df_LDATOPICS_synonym.drop('Unnamed: 0', axis = 1)\n",
    "print(df_LDATOPICS_synonym)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a function to produce test train splits for each of the datasets we are going to use for our boosting algorithm. Since we set the same random_seed for all the data as we use the test_trainsets function, our test and train sets will be consistent for all of the datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def test_trainsets(df):\n",
    "    rating = df['IMDbRating']\n",
    "    xdf = df.drop('IMDbRating', axis=1, inplace=False)\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(xdf, rating, test_size=0.2, random_state=1)\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    y_train_df = pd.DataFrame(y_train)\n",
    "    y_test_df = pd.DataFrame(y_test)\n",
    "\n",
    "    return X_train_df, y_train_df, X_test_df, y_test_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "#ROOM FOR PARAMETER TESTING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly we train the LDA model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7265008439039338\n",
      "Baseline: 0.8036852310907336\n",
      "MAE: 0.7397469437282511\n",
      "Baseline: 0.8222375264785754\n",
      "MAE: 0.7114159764804218\n",
      "Baseline: 0.7977755235768447\n",
      "MAE: 0.6829293358577647\n",
      "Baseline: 0.7885243492376062\n",
      "MAE: 0.7136890634694163\n",
      "Baseline: 0.8163850941331495\n",
      "Average MAE over training: 0.7148564326879576\n",
      "Average Baseline over training: 0.8057215449033819\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = test_trainsets(df_LDATOPICS)\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# store the score for each fold\n",
    "scores = []\n",
    "\n",
    "mae_scores = []\n",
    "baseline_scores = []\n",
    "# Iterate over the folds\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred_fold = xgb_model.predict(X_val_fold)\n",
    "    # Compute the MAE score\n",
    "    baseline = mean_absolute_error(y_val_fold, np.array([np.mean(y_train['IMDbRating'])] * len(y_pred_fold)))\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"Baseline: {baseline}\")\n",
    "    mae_scores.append(mae)\n",
    "    baseline_scores.append(baseline)\n",
    "\n",
    "# Compute the average MAE score\n",
    "average_mae_LDA = sum(mae_scores) / len(mae_scores)\n",
    "average_baseline = sum(baseline_scores) / len(baseline_scores)\n",
    "print(f\"Average MAE over training: {average_mae_LDA}\")\n",
    "print(f\"Average Baseline over training: {average_baseline}\")\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "test_score_LDA = mean_absolute_error(y_pred, y_test)\n",
    "results = pd.DataFrame({'Actual': y_test['IMDbRating'], 'Prediction': y_pred})\n",
    "results.to_csv('LDA_Topics_results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we train our LDA model with synonyms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7303719280092966\n",
      "Baseline: 0.8036852310907336\n",
      "MAE: 0.7207185729680272\n",
      "Baseline: 0.8222375264785754\n",
      "MAE: 0.698293873536152\n",
      "Baseline: 0.7977755235768447\n",
      "MAE: 0.6922140400148857\n",
      "Baseline: 0.7885243492376062\n",
      "MAE: 0.6980984519402034\n",
      "Baseline: 0.8163850941331495\n",
      "Average MAE over training: 0.7079393732937129\n",
      "Average Baseline over training: 0.8057215449033819\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = test_trainsets(df_LDATOPICS_synonym)\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# store the score for each fold\n",
    "scores = []\n",
    "\n",
    "mae_scores = []\n",
    "baseline_scores = []\n",
    "# Iterate over the folds\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred_fold = xgb_model.predict(X_val_fold)\n",
    "    # Compute the MAE score\n",
    "    baseline = mean_absolute_error(y_val_fold, np.array([np.mean(y_train['IMDbRating'])] * len(y_pred_fold)))\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"Baseline: {baseline}\")\n",
    "    mae_scores.append(mae)\n",
    "    baseline_scores.append(baseline)\n",
    "\n",
    "# Compute the average MAE score\n",
    "average_mae_LDASynonym = sum(mae_scores) / len(mae_scores)\n",
    "average_baseline = sum(baseline_scores) / len(baseline_scores)\n",
    "\n",
    "print(f\"Average MAE over training: {average_mae_LDASynonym}\")\n",
    "print(f\"Average Baseline over training: {average_baseline}\")\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "test_score_LDA_synonym = mean_absolute_error(y_pred, y_test)\n",
    "results = pd.DataFrame({'Actual': y_test['IMDbRating'], 'Prediction': y_pred})\n",
    "results.to_csv('LDA_Topics_synonym_results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We train our HF pre-trained text encoder model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7503356624083918\n",
      "Baseline: 0.8036852310907336\n",
      "MAE: 0.7656118661108036\n",
      "Baseline: 0.8222375264785754\n",
      "MAE: 0.7458166718254162\n",
      "Baseline: 0.7977755235768447\n",
      "MAE: 0.7168532994338052\n",
      "Baseline: 0.7885243492376062\n",
      "MAE: 0.7466297551446135\n",
      "Baseline: 0.8163850941331495\n",
      "Average MAE over training: 0.7450494509846061\n",
      "Average Baseline over training: 0.8057215449033819\n"
     ]
    }
   ],
   "source": [
    "df_HFTransformer = pd.read_csv(\"Data/PreProcessedData_with_HF_embeddings.csv\")\n",
    "X_train, y_train, X_test, y_test = test_trainsets(df_HFTransformer)\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# store the score for each fold\n",
    "scores = []\n",
    "\n",
    "mae_scores = []\n",
    "baseline_scores = []\n",
    "# Iterate over the folds\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred_fold = xgb_model.predict(X_val_fold)\n",
    "    # Compute the MAE score\n",
    "    baseline = mean_absolute_error(y_val_fold, np.array([np.mean(y_train['IMDbRating'])] * len(y_pred_fold)))\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"Baseline: {baseline}\")\n",
    "    mae_scores.append(mae)\n",
    "    baseline_scores.append(baseline)\n",
    "\n",
    "# Compute the average MAE score\n",
    "average_mae_HF = sum(mae_scores) / len(mae_scores)\n",
    "average_baseline = sum(baseline_scores) / len(baseline_scores)\n",
    "\n",
    "print(f\"Average MAE over training: {average_mae_HF}\")\n",
    "print(f\"Average Baseline over training: {average_baseline}\")\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "test_score_HF = mean_absolute_error(y_pred, y_test)\n",
    "results = pd.DataFrame({'Actual': y_test['IMDbRating'], 'Prediction': y_pred})\n",
    "\n",
    "results.to_csv('HF_Transformer_Model_Results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we train our boosting model which does not use any text data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7011224842711582\n",
      "Baseline: 0.8036852310907336\n",
      "MAE: 0.7171122579794241\n",
      "Baseline: 0.8222375264785754\n",
      "MAE: 0.6979645785351861\n",
      "Baseline: 0.7977755235768447\n",
      "MAE: 0.6693779420028949\n",
      "Baseline: 0.7885243492376062\n",
      "MAE: 0.6936342674116255\n",
      "Baseline: 0.8163850941331495\n",
      "Average MAE over training: 0.6958423060400578\n",
      "Average Baseline over training: 0.8057215449033819\n"
     ]
    }
   ],
   "source": [
    "# Baseline boost model\n",
    "\n",
    "df_BaselineBoost = pd.read_csv(\"Data/PreProcessedData.csv\")\n",
    "df_BaselineBoost = df_BaselineBoost.drop('Plot', axis=1)\n",
    "df_BaselineBoost = df_BaselineBoost.drop('Title', axis=1)\n",
    "X_train, y_train, X_test, y_test = test_trainsets(df_BaselineBoost)\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# store the score for each fold\n",
    "scores = []\n",
    "\n",
    "mae_scores = []\n",
    "baseline_scores = []\n",
    "# Iterate over the folds\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred_fold = xgb_model.predict(X_val_fold)\n",
    "    # Compute the MAE score\n",
    "    baseline = mean_absolute_error(y_val_fold, np.array([np.mean(y_train['IMDbRating'])] * len(y_pred_fold)))\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"Baseline: {baseline}\")\n",
    "    mae_scores.append(mae)\n",
    "    baseline_scores.append(baseline)\n",
    "\n",
    "# Compute the average MAE score\n",
    "average_mae_baselineboost = sum(mae_scores) / len(mae_scores)\n",
    "average_baseline = sum(baseline_scores) / len(baseline_scores)\n",
    "\n",
    "print(f\"Average MAE over training: {average_mae_baselineboost}\")\n",
    "print(f\"Average Baseline over training: {average_baseline}\")\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "test_score_baselineboost = mean_absolute_error(y_pred, y_test)\n",
    "results = pd.DataFrame({'Actual': y_test['IMDbRating'], 'Prediction': y_pred})\n",
    "\n",
    "results.to_csv('BaselineBoost_Model_Results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING AVERAGE MAE\n",
      "HuggingFaceTransformer: 0.7450494509846061, LDA: 0.7148564326879576, LDA with synonyms: 0.7079393732937129, BaselineBoostingModel: 0.6958423060400578, Baseline: 0.8057215449033819\n",
      "TEST MAE\n",
      "HuggingFaceTransformer: 0.74668775947846, LDA: 0.7180114193870211, LDA with synonyms: 0.7227935320033719, BaselineBoostingModel: 0.6949550314308219, Baseline: 0.817787598198688\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING AVERAGE MAE\")\n",
    "print(f\"HuggingFaceTransformer: {average_mae_HF}, LDA: {average_mae_LDA}, LDA with synonyms: {average_mae_LDASynonym}, BaselineBoostingModel: {average_mae_baselineboost}, Baseline: {average_baseline}\")\n",
    "print(\"TEST MAE\")\n",
    "print(f\"HuggingFaceTransformer: {test_score_HF}, LDA: {test_score_LDA}, LDA with synonyms: {test_score_LDA_synonym}, BaselineBoostingModel: {test_score_baselineboost}, Baseline: {mean_absolute_error(y_test, np.array([np.mean(y_train['IMDbRating'])] * len(y_test)))}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now evaluate the performance of these models in the next section."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
