{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# XGBoost Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We choose to implement an XGBoost model since it has been a high performance model in our previous data science experience. For our rating prediction task we want the model to have the ability to handle complex data as well as being fast and efficient.\n",
    "\n",
    "For our loss function we choose to use MeanAbsoluteError(MAE). This is a standard loss function for this task and does not penalise outlier data as strongly as MeanSquaredError(MSE)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We begin by concatenating our LDA models with the output dataset from 02, These models have not used the 'Title' column in our dataset since the short titles of films felt as if they would be weak predictors, except in the rare case of sequels. The original 'Plot' and 'Title' column are then dropped so the data is now in a format which can be input for the XGBoost model.\n",
    "\n",
    "For an XGBoost model, we do not need to normalize our data since the model is gradient boosted decision trees."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Data/PreProcessedData.csv\")\n",
    "df_no_plot = df1.drop('Plot', axis = 1)\n",
    "df2 = pd.read_csv(\"Data/LDA_topics.csv\") #UPDATE THIS TO BE WHATEVER DATA WE GIVE IT\n",
    "df_LDATOPICS = pd.concat([df_no_plot, df2], axis=1)\n",
    "df_LDATOPICS = df_LDATOPICS.drop('Title', axis = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"Data/LDA_topics_synonym.csv\")\n",
    "df_LDATOPICS_synonym = pd.concat([df_no_plot, df2], axis=1)\n",
    "df_LDATOPICS_synonym = df_LDATOPICS_synonym.drop('Title', axis = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a function to produce test train splits for each of the datasets we are going to use for our boosting algorithm. Since we set the same random_seed for all the data as we use the test_trainsets function, our test and train sets will be consistent for all of the datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def test_trainsets(df):\n",
    "    rating = df['IMDbRating']\n",
    "    xdf = df.drop('IMDbRating', axis=1, inplace=False)\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(xdf, rating, test_size=0.2, random_state=1)\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train)\n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    y_train_df = pd.DataFrame(y_train)\n",
    "    y_test_df = pd.DataFrame(y_test)\n",
    "\n",
    "    return X_train_df, y_train_df, X_test_df, y_test_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#ROOM FOR PARAMETER TESTING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly we train the LDA model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7274074744629608\n",
      "Baseline: 0.8036852310907336\n",
      "MAE: 0.7213504765175583\n",
      "Baseline: 0.8222375264785754\n",
      "MAE: 0.7039843221543617\n",
      "Baseline: 0.7977755235768447\n",
      "MAE: 0.6860378799877789\n",
      "Baseline: 0.7885243492376062\n",
      "MAE: 0.7165792621715055\n",
      "Baseline: 0.8163850941331495\n",
      "Average MAE over training: 0.7110718830588331\n",
      "Average Baseline over training: 0.8057215449033819\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = test_trainsets(df_LDATOPICS)\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# store the score for each fold\n",
    "scores = []\n",
    "\n",
    "mae_scores = []\n",
    "baseline_scores = []\n",
    "# Iterate over the folds\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred_fold = xgb_model.predict(X_val_fold)\n",
    "    # Compute the MAE score\n",
    "    baseline = mean_absolute_error(y_val_fold, np.array([np.mean(y_train['IMDbRating'])] * len(y_pred_fold)))\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"Baseline: {baseline}\")\n",
    "    mae_scores.append(mae)\n",
    "    baseline_scores.append(baseline)\n",
    "\n",
    "# Compute the average MAE score\n",
    "average_mae_LDA = sum(mae_scores) / len(mae_scores)\n",
    "average_baseline = sum(baseline_scores) / len(baseline_scores)\n",
    "print(f\"Average MAE over training: {average_mae_LDA}\")\n",
    "print(f\"Average Baseline over training: {average_baseline}\")\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "test_score_LDA = mean_absolute_error(y_pred, y_test)\n",
    "results = pd.DataFrame({'Actual': y_test['IMDbRating'], 'Prediction': y_pred})\n",
    "results.to_csv('LDA_Topics_results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7254401595114748\n",
      "Baseline: 0.8036852310907336\n",
      "MAE: 0.7294935720209426\n",
      "Baseline: 0.8222375264785754\n",
      "MAE: 0.6960031597719541\n",
      "Baseline: 0.7977755235768447\n",
      "MAE: 0.697834530352631\n",
      "Baseline: 0.7885243492376062\n",
      "MAE: 0.6945821478362275\n",
      "Baseline: 0.8163850941331495\n",
      "Average MAE over training: 0.708670713898646\n",
      "Average Baseline over training: 0.8057215449033819\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = test_trainsets(df_LDATOPICS_synonym)\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# store the score for each fold\n",
    "scores = []\n",
    "\n",
    "mae_scores = []\n",
    "baseline_scores = []\n",
    "# Iterate over the folds\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred_fold = xgb_model.predict(X_val_fold)\n",
    "    # Compute the MAE score\n",
    "    baseline = mean_absolute_error(y_val_fold, np.array([np.mean(y_train['IMDbRating'])] * len(y_pred_fold)))\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"Baseline: {baseline}\")\n",
    "    mae_scores.append(mae)\n",
    "    baseline_scores.append(baseline)\n",
    "\n",
    "# Compute the average MAE score\n",
    "average_mae_LDASynonym = sum(mae_scores) / len(mae_scores)\n",
    "average_baseline = sum(baseline_scores) / len(baseline_scores)\n",
    "\n",
    "print(f\"Average MAE over training: {average_mae_LDASynonym}\")\n",
    "print(f\"Average Baseline over training: {average_baseline}\")\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "test_score_LDA_synonym = mean_absolute_error(y_pred, y_test)\n",
    "results = pd.DataFrame({'Actual': y_test['IMDbRating'], 'Prediction': y_pred})\n",
    "results.to_csv('LDA_Topics_synonym_results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7503356624083918\n",
      "Baseline: 0.8036852310907336\n",
      "MAE: 0.7656118661108036\n",
      "Baseline: 0.8222375264785754\n",
      "MAE: 0.7458166718254162\n",
      "Baseline: 0.7977755235768447\n",
      "MAE: 0.7168532994338052\n",
      "Baseline: 0.7885243492376062\n",
      "MAE: 0.7466297551446135\n",
      "Baseline: 0.8163850941331495\n",
      "Average MAE over training: 0.7450494509846061\n",
      "Average Baseline over training: 0.8057215449033819\n"
     ]
    }
   ],
   "source": [
    "df_HFTransformer = pd.read_csv(\"Data/PreProcessedData_with_HF_embeddings.csv\")\n",
    "X_train, y_train, X_test, y_test = test_trainsets(df_HFTransformer)\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# store the score for each fold\n",
    "scores = []\n",
    "\n",
    "mae_scores = []\n",
    "baseline_scores = []\n",
    "# Iterate over the folds\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred_fold = xgb_model.predict(X_val_fold)\n",
    "    # Compute the MAE score\n",
    "    baseline = mean_absolute_error(y_val_fold, np.array([np.mean(y_train['IMDbRating'])] * len(y_pred_fold)))\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"Baseline: {baseline}\")\n",
    "    mae_scores.append(mae)\n",
    "    baseline_scores.append(baseline)\n",
    "\n",
    "# Compute the average MAE score\n",
    "average_mae_HF = sum(mae_scores) / len(mae_scores)\n",
    "average_baseline = sum(baseline_scores) / len(baseline_scores)\n",
    "\n",
    "print(f\"Average MAE over training: {average_mae_HF}\")\n",
    "print(f\"Average Baseline over training: {average_baseline}\")\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "test_score_HF = mean_absolute_error(y_pred, y_test)\n",
    "results = pd.DataFrame({'Actual': y_test['IMDbRating'], 'Prediction': y_pred})\n",
    "\n",
    "results.to_csv('HF_Transformer_Model_Results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7011224842711582\n",
      "Baseline: 0.8036852310907336\n",
      "MAE: 0.7171122579794241\n",
      "Baseline: 0.8222375264785754\n",
      "MAE: 0.6979645785351861\n",
      "Baseline: 0.7977755235768447\n",
      "MAE: 0.6693779420028949\n",
      "Baseline: 0.7885243492376062\n",
      "MAE: 0.6936342674116255\n",
      "Baseline: 0.8163850941331495\n",
      "Average MAE over training: 0.6958423060400578\n",
      "Average Baseline over training: 0.8057215449033819\n"
     ]
    }
   ],
   "source": [
    "# Baseline boost model\n",
    "\n",
    "df_BaselineBoost = pd.read_csv(\"Data/PreProcessedData.csv\")\n",
    "df_BaselineBoost = df_BaselineBoost.drop('Plot', axis=1)\n",
    "df_BaselineBoost = df_BaselineBoost.drop('Title', axis=1)\n",
    "X_train, y_train, X_test, y_test = test_trainsets(df_BaselineBoost)\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Kfold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# store the score for each fold\n",
    "scores = []\n",
    "\n",
    "mae_scores = []\n",
    "baseline_scores = []\n",
    "# Iterate over the folds\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model on the training data\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_pred_fold = xgb_model.predict(X_val_fold)\n",
    "    # Compute the MAE score\n",
    "    baseline = mean_absolute_error(y_val_fold, np.array([np.mean(y_train['IMDbRating'])] * len(y_pred_fold)))\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"Baseline: {baseline}\")\n",
    "    mae_scores.append(mae)\n",
    "    baseline_scores.append(baseline)\n",
    "\n",
    "# Compute the average MAE score\n",
    "average_mae_baselineboost = sum(mae_scores) / len(mae_scores)\n",
    "average_baseline = sum(baseline_scores) / len(baseline_scores)\n",
    "\n",
    "print(f\"Average MAE over training: {average_mae_baselineboost}\")\n",
    "print(f\"Average Baseline over training: {average_baseline}\")\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "test_score_baselineboost = mean_absolute_error(y_pred, y_test)\n",
    "results = pd.DataFrame({'Actual': y_test['IMDbRating'], 'Prediction': y_pred})\n",
    "\n",
    "results.to_csv('BaselineBoost_Model_Results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING AVERAGE MAE\n",
      "HuggingFaceTransformer: 0.7450494509846061, LDA: 0.7110718830588331, LDA with synonyms: 0.708670713898646, BaselineBoostingModel: 0.6958423060400578, Baseline: 0.8057215449033819\n",
      "TEST MAE\n",
      "HuggingFaceTransformer: 0.74668775947846, LDA: 0.7066671046859379, LDA with synonyms: 0.7162575727595976, BaselineBoostingModel: 0.6949550314308219, Baseline: 0.817787598198688\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING AVERAGE MAE\")\n",
    "print(f\"HuggingFaceTransformer: {average_mae_HF}, LDA: {average_mae_LDA}, LDA with synonyms: {average_mae_LDASynonym}, BaselineBoostingModel: {average_mae_baselineboost}, Baseline: {average_baseline}\")\n",
    "print(\"TEST MAE\")\n",
    "print(f\"HuggingFaceTransformer: {test_score_HF}, LDA: {test_score_LDA}, LDA with synonyms: {test_score_LDA_synonym}, BaselineBoostingModel: {test_score_baselineboost}, Baseline: {mean_absolute_error(y_test, np.array([np.mean(y_train['IMDbRating'])] * len(y_test)))}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
