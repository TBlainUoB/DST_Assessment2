{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a656fc",
   "metadata": {},
   "source": [
    "# Topic Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "682a4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9979e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a45d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec63af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "df = pd.read_csv('../Tom/Movie_database_BritishAmerican2000-2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dad0354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Kaisa is a Scot, a successful London lawyer, w...\n",
       "1       Plagued by endless visions and nightmares, Jen...\n",
       "2       Garland's novel centers on a young nicotine-ad...\n",
       "3       In the Bronx, Joe (Sir Billy Connolly), an Iri...\n",
       "4       A woman who, by a promise made years earlier, ...\n",
       "                              ...                        \n",
       "9264    Puss in Boots discovers that his passion for a...\n",
       "9265                                                  NaN\n",
       "9266    A tale of outsized ambition and outrageous exc...\n",
       "9267    A joyous, emotional, heartbreaking celebration...\n",
       "9268    At West Point Academy in 1830, the calm of an ...\n",
       "Name: Plot, Length: 9269, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing dataset\n",
    "df\n",
    "df['Plot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6154f55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9269\n",
      "   IMDbRating                     Title  Year                      Genre  \\\n",
      "0         7.1                  Aberdeen  2000                      Drama   \n",
      "1         4.1                The Asylum  2000    Drama, Horror, Thriller   \n",
      "2         6.6                 The Beach  2000  Adventure, Drama, Romance   \n",
      "3         5.6             Beautiful Joe  2000              Comedy, Drama   \n",
      "4         6.3  My Best Friend's Wedding  1997     Comedy, Drama, Romance   \n",
      "\n",
      "                                                Plot  \\\n",
      "0  Kaisa is a Scot, a successful London lawyer, w...   \n",
      "1  Plagued by endless visions and nightmares, Jen...   \n",
      "2  Garland's novel centers on a young nicotine-ad...   \n",
      "3  In the Bronx, Joe (Sir Billy Connolly), an Iri...   \n",
      "4  A woman who, by a promise made years earlier, ...   \n",
      "\n",
      "                                           Actors  \n",
      "0  Stellan Skarsgård, Lena Headey, Jean Johansson  \n",
      "1        Steffanie Pitt, Nick Waring, Ingrid Pitt  \n",
      "2   Leonardo DiCaprio, Tilda Swinton, Daniel York  \n",
      "3       Sharon Stone, Billy Connolly, Gil Bellows  \n",
      "4    Julia Roberts, Dermot Mulroney, Cameron Diaz  \n"
     ]
    }
   ],
   "source": [
    "#Remove NAs\n",
    "df. dropna()\n",
    "#EDA \n",
    "print(len(df)) \n",
    "print(df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507129bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stop words \n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace797ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From workshop - setting up lemmatisation and removing stop words \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lmtzr = nltk.WordNetLemmatizer().lemmatize\n",
    "\n",
    "## We lookup whether a word is and adjective, verb, noun or adverb here.\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    \n",
    "## This version uses word type. Needs the bigger nltp download (\"popular\")\n",
    "def normalize_text(text):\n",
    "    ## Runs on documents (vector of words)\n",
    "    word_pos = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    lemm_words = [lmtzr(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n",
    "\n",
    "    return [x.lower() for x in lemm_words]\n",
    "\n",
    "## This version doesn't require the \"popular\" download\n",
    "def preprocess(text):\n",
    "    ## Runs on documents (vector of words)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    return([lemmatizer.lemmatize(i) for i in text.split()])\n",
    "\n",
    "################\n",
    "## wordnet version\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    ## morphy does a lemma lookup and word standardization\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "## lemmatize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "## This version is for comparison\n",
    "def prepare_text_for_lda(text):\n",
    "    ## Runs on documents (vector of words)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8d9995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plagued by endless visions and nightmares, Jenny Adams suspects that, as a child, she was responsible for the brutal murder of her own mother.\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1]['Plot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "079d80ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Plagued', 'by', 'endless', 'visions', 'and', 'nightmares,', 'Jenny', 'Adams', 'suspects', 'that,', 'as', 'a', 'child,', 'she', 'was', 'responsible', 'for', 'the', 'brutal', 'murder', 'of', 'her', 'own', 'mother.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['plagued', 'by', 'endless', 'vision', 'and', 'nightmare', ',', 'jenny', 'adams', 'suspect', 'that', ',', 'a', 'a', 'child', ',', 'she', 'be', 'responsible', 'for', 'the', 'brutal', 'murder', 'of', 'her', 'own', 'mother', '.']\n",
      "\n",
      "\n",
      " simpler tokenized and lemmatized document: \n",
      "['Plagued', 'by', 'endless', 'vision', 'and', 'nightmares,', 'Jenny', 'Adams', 'suspect', 'that,', 'a', 'a', 'child,', 'she', 'wa', 'responsible', 'for', 'the', 'brutal', 'murder', 'of', 'her', 'own', 'mother.']\n",
      "\n",
      "\n",
      " method removing stop words: \n",
      "['Plagued', 'endless', 'vision', 'nightmare', 'Jenny', 'Adams', 'suspect', 'child', 'responsible', 'brutal', 'murder', 'mother']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = df.iloc[1]['Plot']\n",
    "\n",
    "from gensim import parsing\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(normalize_text(doc_sample))\n",
    "print('\\n\\n simpler tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n",
    "print('\\n\\n method removing stop words: ')\n",
    "print(prepare_text_for_lda(doc_sample))\n",
    "\n",
    "# The method removing stopwords appears successful but the lemmatisation is pretty bad in all cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da758bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "[\"Garland's\", 'novel', 'centers', 'on', 'a', 'young', 'nicotine-addicted', 'traveler', 'named', 'Richard,', 'an', 'avid', 'pop-culture', 'buff', 'with', 'a', 'particular', 'love', 'for', 'video', 'games', 'and', 'Vietnam', 'War', 'movies.', 'While', 'at', 'a', 'hotel', 'in', 'Bangkok,', 'he', 'finds', 'a', 'map', 'left', 'by', 'his', 'strange,', 'whacked-out', 'neighbor,', 'who', 'just', 'committed', 'suicide.', 'The', 'map', 'supposedly', 'leads', 'to', 'a', 'legendary', 'island', 'paradise', 'where', 'some', 'other', 'wayward', 'souls', 'have', 'settled.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['garland', \"'s\", 'novel', 'center', 'on', 'a', 'young', 'nicotine-addicted', 'traveler', 'name', 'richard', ',', 'an', 'avid', 'pop-culture', 'buff', 'with', 'a', 'particular', 'love', 'for', 'video', 'game', 'and', 'vietnam', 'war', 'movie', '.', 'while', 'at', 'a', 'hotel', 'in', 'bangkok', ',', 'he', 'find', 'a', 'map', 'leave', 'by', 'his', 'strange', ',', 'whacked-out', 'neighbor', ',', 'who', 'just', 'commit', 'suicide', '.', 'the', 'map', 'supposedly', 'lead', 'to', 'a', 'legendary', 'island', 'paradise', 'where', 'some', 'other', 'wayward', 'soul', 'have', 'settle', '.']\n",
      "\n",
      "\n",
      " simpler tokenized and lemmatized document: \n",
      "[\"Garland's\", 'novel', 'center', 'on', 'a', 'young', 'nicotine-addicted', 'traveler', 'named', 'Richard,', 'an', 'avid', 'pop-culture', 'buff', 'with', 'a', 'particular', 'love', 'for', 'video', 'game', 'and', 'Vietnam', 'War', 'movies.', 'While', 'at', 'a', 'hotel', 'in', 'Bangkok,', 'he', 'find', 'a', 'map', 'left', 'by', 'his', 'strange,', 'whacked-out', 'neighbor,', 'who', 'just', 'committed', 'suicide.', 'The', 'map', 'supposedly', 'lead', 'to', 'a', 'legendary', 'island', 'paradise', 'where', 'some', 'other', 'wayward', 'soul', 'have', 'settled.']\n",
      "\n",
      "\n",
      " method removing stop words: \n",
      "['Garland', 'novel', 'center', 'young', 'nicotine-addicted', 'traveler', 'name', 'Richard', 'pop-culture', 'particular', 'video', 'game', 'Vietnam', 'movie', 'While', 'hotel', 'Bangkok', 'find', 'strange', 'whacked-out', 'neighbor', 'commit', 'suicide', 'supposedly', 'lead', 'legendary', 'island', 'paradise', 'wayward', 'soul', 'settle']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = df.iloc[2]['Plot']\n",
    "\n",
    "from gensim import parsing\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(normalize_text(doc_sample))\n",
    "print('\\n\\n simpler tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n",
    "print('\\n\\n method removing stop words: ')\n",
    "print(prepare_text_for_lda(doc_sample))\n",
    "\n",
    "#Again removing stop words looks good\n",
    "# first lemmatisation seems to be doing better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc0e13b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Kaisa, is, a, Scot,, a, successful, London, l...\n",
       "1    [Plagued, by, endless, vision, and, nightmares...\n",
       "2    [Garland's, novel, center, on, a, young, nicot...\n",
       "3    [In, the, Bronx,, Joe, (Sir, Billy, Connolly),...\n",
       "4    [A, woman, who,, by, a, promise, made, year, e...\n",
       "5    [County, Durham,, during, the, endless,, viole...\n",
       "6    [The, intersecting, life, story, of, Daniel, P...\n",
       "7    [Brendan, Behan,, a, sixteen, year-old, republ...\n",
       "8    [Maya, is, a, quick-witted, young, woman, who,...\n",
       "9    [A, beautiful, psychiatrist, befriends, an, ab...\n",
       "Name: Plot, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = df[df['Plot'].notnull()]\n",
    "processed_df = df_new['Plot'].map(preprocess) # preprocess is faster than normalise_text.\n",
    "processed_df[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k,v  in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be58b7",
   "metadata": {},
   "source": [
    "## https://github.com/rfhussain/Topic-Modeling-with-Python-Scikit-LDA/blob/master/source/lda_test.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3c471f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d311285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the count vectorizer\n",
    "#max document frequencey means that the percentage of max frequency shuld be less than 90% of any word across documents\n",
    "#min document frequencey is an integer, means that a word must occur at least 2 or more times to be counted\n",
    "#stop words will be automatically tackled through sklearn \n",
    "cv = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62fa0e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the fit transform method will return a sparse matrix (numberofariticles x totalwords)\n",
    "dtm  = cv.fit_transform(df_new['Plot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74af97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the LDA, n_components =10 means that we are opting for 10 distinct topics\n",
    "#the n_components depends upon how big is the repository and how many topics you want to discover\n",
    "#keep the random state as 42\n",
    "LDA = LatentDirichletAllocation(n_components=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea6a520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model into lda\n",
    "LDA.fit(dtm)\n",
    "\n",
    "#grab the vocabulary of words\n",
    "#get the random words \n",
    "random_int = random.randint(0,5477)\n",
    "\n",
    "cv.get_feature_names()[random_int] #this function will get the words from the document\n",
    "\n",
    "#grab the topics\n",
    "single_topic = LDA.components_[0]\n",
    "\n",
    "\n",
    "#this way we can get index position for high probablity topics SORTED by probablity in ASC order\n",
    "top_10_words = single_topic.argsort()[-10:] #to get the last 10 highest probablity words for this topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5fb624e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "gets\n",
      "does\n",
      "team\n",
      "langford\n",
      "perform\n",
      "host\n",
      "obsessed\n",
      "talk\n",
      "jerry\n",
      "The top 15 words for the topic #0\n",
      "['great', 'gets', 'does', 'team', 'langford', 'perform', 'host', 'obsessed', 'talk', 'jerry']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for the topic #1\n",
      "['school', 'max', 'wife', 'high', 'money', 'son', 'los', 'angeles', 'home', 'family']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for the topic #2\n",
      "['man', 'young', 'family', 'film', 'new', 'set', 'love', 'world', 'life', 'story']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for the topic #3\n",
      "['house', 'story', 'salvation', 'luck', 'drug', 'wife', 'reverend', 'jr', 'grace', 'salo']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for the topic #4\n",
      "['beachum', 'wants', 'time', 'soon', 'car', 'death', 'louis', 'bob', 'frank', 'steve']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for the topic #5\n",
      "['future', 'town', 'report', 'help', 'world', 'man', 'ted', 'school', 'story', 'life']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for the topic #6\n",
      "['father', 'love', 'family', 'year', 'high', 'friend', 'friends', 'school', 'new', 'life']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for the topic #7\n",
      "['mission', 'time', 'country', 'love', 'years', 'life', 'finds', 'agent', 'family', 'young']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for the topic #8\n",
      "['save', 'finds', 'town', 'old', 'young', 'new', 'time', 'life', 'world', 'man']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The top 15 words for the topic #9\n",
      "['obsession', 'stage', 'theater', 'experiment', 'begin', 'lives', 'searching', 'french', 'professor', 'students']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00169558, 0.00169542, 0.37159081, 0.00169507, 0.37041398,\n",
       "       0.00169539, 0.00169552, 0.24612781, 0.00169541, 0.00169501])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index in top_10_words:\n",
    "    print(cv.get_feature_names()[index])\n",
    "    \n",
    "#grab the highest probablity words per topic\n",
    "for i, topic in enumerate(LDA.components_):\n",
    "    print(f\"The top 15 words for the topic #{i}\")\n",
    "    print([cv.get_feature_names()[index] for index in topic.argsort()[-10:]]) \n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "#attach the topic number to the original topics\n",
    "topic_results = LDA.transform(dtm)\n",
    "\n",
    "topic_results[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
