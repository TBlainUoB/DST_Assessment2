{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6b1f2285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6516f63",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16452\\4125471731.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbinary_crossentropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \"\"\"\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f74c6b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maya is a quickwitted young woman who comes over the mexican border without papers and makes her way to the la home of her older sister rosa rosa gets maya a job as a janitor a nonunion janitorial service has the contract the foulmouthed supervisor can fire workers on a whim and the serviceworkers union has assigned organizer sam shapiro to bring its justice for janitors campaign to the building sam finds maya a willing listener shes also attracted to him rosa resists she has an ailing husband to consider the workers try for public support management intimidates workers to divide and conquer rosa and maya as well as workers and management may be set to collide'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('Movie_database_BritishAmerican2000-2021.csv')\n",
    "plots=data.Plot\n",
    "test=plots[8]\n",
    "test=re.sub(r'[^\\w\\s]', '', test)\n",
    "test=test.lower()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "33f41241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#Switching out hyphons for spaces\n",
    "plots=[str(plots[num]).replace('-',' ') for num in range(len(plots))]\n",
    "# Removing punctuation\n",
    "plots=[re.sub(r'[^\\w\\s]','', str(plots[num])) for num in range(len(plots))]\n",
    "# Lowercasing the words\n",
    "plots=[str(plots[num]).lower() for num in range(len(plots))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6f63024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Removing stop words and tokenising\n",
    "\n",
    "#Define english stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Define function for removing stop words + tokenizing\n",
    "def stopntokenize(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    new_text = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            new_text.append(w)\n",
    "    return(new_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0e78e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming and Lemmatizing\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "Ps=PorterStemmer()\n",
    "\n",
    "#Only seems to work well for verbs lol\n",
    "\n",
    "def verb_stemmer(tokens):\n",
    "    wordtypes = nltk.pos_tag(tokens)\n",
    "    for i in range(len(tokens)):\n",
    "        if wordtypes[i][1] in ['VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "            tokens[i] = Ps.stem(tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "47ffe75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "1e9f5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only including important words, (i.e. nouns, verbs, adjectives and adverbs)\n",
    "#['NN','NNP','JJ','JJR','JJS','RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "\n",
    "def wordremover(tokens):\n",
    "    wordtypes = nltk.pos_tag(tokens)\n",
    "    tokens_new=[]\n",
    "    for i in range(len(tokens)):\n",
    "        if wordtypes[i][1] in ['NN','NNP','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "            tokens_new.append(tokens[i])\n",
    "    return(tokens_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "82e0796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def lemmanstem(tokens):\n",
    "    new_tokens=[]\n",
    "    for i in range(len(tokens)):\n",
    "        if wordnet.morphy(tokens[i])==None:\n",
    "            new_tokens.append(tokens[i])\n",
    "        else:\n",
    "            new_tokens.append(wordnet.morphy(tokens[i]))\n",
    "    return(new_tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0ad99c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    token=stopntokenize(text)\n",
    "    token=wordremover(token)\n",
    "    token=lemmanstem(token)\n",
    "    #token=[lemmatizer.lemmatize(token[num]) for num in range(len(token))]\n",
    "    #verb_stemmer(token)\n",
    "    token=list(dict.fromkeys(token))\n",
    "    return(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "697f8322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pierce brosnan gives one last mission as james bond 007 starting off in north korea bond is betrayed and captured fourteen months later bond is set free but traded for zao rick yune who was captured by mi6 when back in his world bond sets off to track down zao bond gets caught up in yet another scheme which sends him to millionaire gustav graves toby stephens another mi6 agent known as miranda frost rosamund pike is also posing as a friend of graves bond is invited to a presentation held by graves about a satellite found in space which can project a huge laser beam bond must stop this madman with a fellow american agent known as jinx johnson halle berry while bond tries to stop graves and zao will he finally reveal who betrayed him'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M=101\n",
    "plots[M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fd2c01cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pierce',\n",
       " 'brosnan',\n",
       " 'give',\n",
       " 'last',\n",
       " 'mission',\n",
       " 'bond',\n",
       " 'starting',\n",
       " 'north',\n",
       " 'korea',\n",
       " 'betray',\n",
       " 'capture',\n",
       " 'fourteen',\n",
       " 'set',\n",
       " 'trade',\n",
       " 'zao',\n",
       " 'rick',\n",
       " 'yune',\n",
       " 'world',\n",
       " 'track',\n",
       " 'get',\n",
       " 'catch',\n",
       " 'scheme',\n",
       " 'send',\n",
       " 'millionaire',\n",
       " 'gustav',\n",
       " 'toby',\n",
       " 'stephen',\n",
       " 'mi6',\n",
       " 'agent',\n",
       " 'know',\n",
       " 'miranda',\n",
       " 'frost',\n",
       " 'rosamund',\n",
       " 'pike',\n",
       " 'posing',\n",
       " 'friend',\n",
       " 'invite',\n",
       " 'presentation',\n",
       " 'hold',\n",
       " 'found',\n",
       " 'space',\n",
       " 'project',\n",
       " 'huge',\n",
       " 'laser',\n",
       " 'beam',\n",
       " 'stop',\n",
       " 'madman',\n",
       " 'fellow',\n",
       " 'american',\n",
       " 'jinx',\n",
       " 'johnson',\n",
       " 'halle',\n",
       " 'berry',\n",
       " 'reveal']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(plots[M])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "75ebe1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07894935929105847"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[]\n",
    "for i in range(4000):\n",
    "    x.append(len(tokenizer(plots[i]))/len(plots[i]))\n",
    "\n",
    "sum(x)/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1c8fd33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09261576971214018"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(tokenizer(plots[5]))/len(plots[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd72c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_plots=[]\n",
    "for i in range(len(plots)):\n",
    "    token_plots.append(tokenizer(plots[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "75eba514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 aberdeen\n",
      "1 agre\n",
      "2 airline\n",
      "3 alcoholic\n",
      "4 along\n",
      "5 ban\n",
      "6 beg\n",
      "7 call\n",
      "8 car\n",
      "9 clive\n",
      "10 coke\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(token_plots)\n",
    "\n",
    "count = 0\n",
    "for k,v  in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "        \n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "db89d1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run=wordnet.synset('run.v.01')\n",
    "sprint=wordnet.synset('sprint.v.01')\n",
    "sprint.path_similarity(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "98f8cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give space\n",
      "starting capture\n",
      "north track\n",
      "capture presentation\n",
      "set space\n",
      "send huge\n",
      "millionaire stephen\n",
      "stephen agent\n",
      "agent friend\n",
      "friend madman\n",
      "presentation hold\n",
      "hold project\n",
      "space huge\n",
      "huge reveal\n",
      "madman fellow\n",
      "fellow american\n",
      "american jinx\n",
      "jinx johnson\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pierce',\n",
       " 'brosnan',\n",
       " 'last',\n",
       " 'mission',\n",
       " 'bond',\n",
       " 'korea',\n",
       " 'betray',\n",
       " 'fourteen',\n",
       " 'trade',\n",
       " 'zao',\n",
       " 'rick',\n",
       " 'yune',\n",
       " 'world',\n",
       " 'track',\n",
       " 'get',\n",
       " 'catch',\n",
       " 'scheme',\n",
       " 'gustav',\n",
       " 'toby',\n",
       " 'mi6',\n",
       " 'know',\n",
       " 'miranda',\n",
       " 'frost',\n",
       " 'rosamund',\n",
       " 'pike',\n",
       " 'posing',\n",
       " 'invite',\n",
       " 'found',\n",
       " 'project',\n",
       " 'laser',\n",
       " 'beam',\n",
       " 'stop',\n",
       " 'johnson',\n",
       " 'halle',\n",
       " 'berry',\n",
       " 'reveal']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=tokenizer(plots[M])\n",
    "for i in range(len(test)):\n",
    "    for j in range(len(test)):\n",
    "        if wordnet.synsets(test[i])!=[] and wordnet.synsets(test[j])!=[]:\n",
    "            \n",
    "            word1=wordnet.synsets(test[i])[0]\n",
    "            word2=wordnet.synsets(test[j])[0]\n",
    "            similarity=word1.path_similarity(word2)\n",
    "            if similarity>0.15 and similarity !=1:\n",
    "                print(test[i],test[j])\n",
    "                test[i]='margaret'\n",
    "new_test=[]\n",
    "for i in range(len(test)):\n",
    "    if test[i]!='margaret':\n",
    "        new_test.append(test[i])\n",
    "new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3513dfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attack', 'assail']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog=wordnet.synset('dog.n.01')\n",
    "dog.lemma_names()\n",
    "wordnet.synset('.v.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "426091a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wordnet.synsets(test[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
