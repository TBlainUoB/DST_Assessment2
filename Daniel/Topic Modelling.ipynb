{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6516f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from keras.layers import Input, Dense, Lambda\n",
    "#from keras.models import Model\n",
    "#from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f74c6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in data frame\n",
    "data=pd.read_csv('C:\\\\Users\\\\Danie\\\\OneDrive\\\\Documents\\\\_Uni\\\\Maths\\\\Year 4\\\\Data Science Toolkit\\\\PreProcessedData.csv')\n",
    "#Creating list of film plots as a list of strings\n",
    "plots=data.Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "33f41241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASIC TEXT EDITS\n",
    "import re\n",
    "#Switching out hyphons for spaces\n",
    "plots=[str(plots[num]).replace('-',' ') for num in range(len(plots))]\n",
    "# Removing punctuation\n",
    "plots=[re.sub(r'[^\\w\\s]','', str(plots[num])) for num in range(len(plots))]\n",
    "# Lowercasing the words\n",
    "plots=[str(plots[num]).lower() for num in range(len(plots))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6f63024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING STOP WORDS AND TOKENIZING\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Define english stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Define function for removing stop words + tokenizing\n",
    "def stopntokenize(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    new_text = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            new_text.append(w)\n",
    "    return(new_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1e9f5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORDREMOVER\n",
    "\n",
    "#Defining function for removing any word not a Noun, Adjective or Verb\n",
    "def wordremover(tokens):\n",
    "    wordtypes = nltk.pos_tag(tokens)\n",
    "    tokens_new=[]\n",
    "    for i in range(len(tokens)):\n",
    "        if wordtypes[i][1] in ['NN','NNP','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "            tokens_new.append(tokens[i])\n",
    "    return(tokens_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "82e0796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEAMMATIZING AND STEMMING\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#Defining function to reduce word down it's simplest form\n",
    "def lemmanstem(tokens):\n",
    "    new_tokens=[]\n",
    "    for i in range(len(tokens)):\n",
    "        if wordnet.morphy(tokens[i])==None:\n",
    "            new_tokens.append(tokens[i])\n",
    "        else:\n",
    "            new_tokens.append(wordnet.morphy(tokens[i]))\n",
    "    return(new_tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8dd49548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NONE REMOVER\n",
    "\n",
    "#Defining function to remove Nones\n",
    "def noneremover(tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i]==None:\n",
    "            tokens.remove(tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0ad99c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STRING TO TOKENS\n",
    "def tokenizer(text):\n",
    "    token=stopntokenize(text)\n",
    "    token=wordremover(token)\n",
    "    token=lemmanstem(token)\n",
    "    token=list(dict.fromkeys(token))\n",
    "    noneremover(token)\n",
    "    return(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "697f8322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the story of tracy edwards a 24 year old cook on charter boats who became the skipper of the first ever all female crew to enter the whitbread round the world race in 1989'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M=1443\n",
    "plots[M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fd2c01cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['story',\n",
       " 'tracy',\n",
       " 'year',\n",
       " 'old',\n",
       " 'cook',\n",
       " 'charter',\n",
       " 'become',\n",
       " 'first',\n",
       " 'female',\n",
       " 'crew',\n",
       " 'whitbread',\n",
       " 'round',\n",
       " 'world',\n",
       " 'race']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(plots[M])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "75ebe1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07198069794144195"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WORD REDUCTION \n",
    "\n",
    "#Calculating percentage reduction of words for plots and tokenized plots\n",
    "x=[]\n",
    "for i in range(4000):\n",
    "    x.append(len(tokenizer(plots[i]))/len(plots[i]))\n",
    "\n",
    "sum(x)/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1c8fd33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.78775"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA SIZE\n",
    "\n",
    "#Calculating average number of words per tokenized plot\n",
    "#i.e. number of words remaining after text preprocessing\n",
    "x=[]\n",
    "for i in range(4000):\n",
    "    x.append(len(tokenizer(plots[i])))\n",
    "\n",
    "sum(x)/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cd72c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING DATASET OF TOKENIZED PLOTS\n",
    "token_plots=[]\n",
    "for i in range(len(plots)):\n",
    "    token_plots.append(tokenizer(plots[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "75eba514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 aberdeen\n",
      "1 agree\n",
      "2 airline\n",
      "3 alcoholic\n",
      "4 along\n",
      "5 ban\n",
      "6 begging\n",
      "7 car\n",
      "8 clive\n",
      "9 coke\n",
      "10 collect\n"
     ]
    }
   ],
   "source": [
    "#CREATING DICTIONARY\n",
    "import gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(token_plots)\n",
    "\n",
    "count = 0\n",
    "for k,v  in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "        \n",
    "#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e4871ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life', 'find', 'new', 'take', 'get', 'young', 'family', 'world', 'go', 'man']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FINDING TOP WORDS\n",
    "\n",
    "#Creating dictionary of wordcounts\n",
    "new_dict=dictionary.cfs\n",
    "#Sorting by wordcount\n",
    "new_dict2 = sorted(new_dict.items(), key=lambda x:x[1],reverse=True)\n",
    "#Translating word index to word\n",
    "ranked_words=[]\n",
    "for i in range(len(new_dict2)):\n",
    "    ranked_words.append(dictionary[new_dict2[i][0]])\n",
    "#Top 10 words\n",
    "ranked_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "98f8cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING SYNONYM REMOVER\n",
    "def synonymremover(tokens):\n",
    "    #Defining list of unneccesary synonyms\n",
    "    synonyms=[]\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            word1=tokens[i]\n",
    "            word2=tokens[j]\n",
    "            #Get list of what the word could mean\n",
    "            list_of_words1=wordnet.synsets(word1)\n",
    "            list_of_words2=wordnet.synsets(word2)\n",
    "            #Check that word has a possible definition\n",
    "            if not (list_of_words1==[] or list_of_words2==[]):\n",
    "                #Define words as the first choice \n",
    "                word_1=list_of_words1[0]\n",
    "                word_2=list_of_words2[0]\n",
    "                #Caculate similarity\n",
    "                similarity=(word_1).path_similarity(word_2)\n",
    "                #If they are close enough, add less significant word to synonyms\n",
    "                if similarity>0.3 and similarity !=1:\n",
    "                    #Calculate significance by getting their rank from the ranked words\n",
    "                    sig1 = ranked_words.index(word1)\n",
    "                    sig2 = ranked_words.index(word2)\n",
    "                    #Add whichever word is lower down in the ranking to synonyms\n",
    "                    if sig1<sig2:\n",
    "                        synonyms.append(tokens[j])\n",
    "                    else:\n",
    "                        synonyms.append(tokens[i])\n",
    "    #Remove all unneccesary synonyms\n",
    "    for word in tokens:\n",
    "        if word in synonyms:\n",
    "            tokens.remove(word)\n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7fa8f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UPDATING TOKENIZED PLOTS\n",
    "#Takes quite a while atm lol\n",
    "\n",
    "#for i in range(len(token_plots)):\n",
    "#    token_plots[i]=synonymremover(token_plots[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "75b43835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying a simialr thing with the actor names\n",
    "\n",
    "#Getting dataframe of all actors. These are the top 3 billed in the film, separated by a comma and a space\n",
    "actors=data.Actors\n",
    "#Defining function to remove spaces between forname and surname, then switch commas to spaces\n",
    "def namestowords(names):\n",
    "    new_names=names.replace(' ','')\n",
    "    new_names1=new_names.replace(',',' ')\n",
    "    return(new_names1)\n",
    "\n",
    "actors=[namestowords(actors[num]) for num in range(len(actors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cba28062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe of tokenized actors\n",
    "tokenized_actors=[]\n",
    "for i in range(len(actors)):\n",
    "    tokenized_actors.append(word_tokenize(actors[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c183cec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BruceWillis',\n",
       " 'SamuelL.Jackson',\n",
       " 'NicolasCage',\n",
       " 'LiamNeeson',\n",
       " 'MarkWahlberg',\n",
       " 'EwanMcGregor',\n",
       " 'RyanReynolds',\n",
       " 'RobertDeNiro',\n",
       " 'DwayneJohnson',\n",
       " 'OwenWilson',\n",
       " 'MattDamon',\n",
       " 'JohnnyDepp',\n",
       " 'NicoleKidman',\n",
       " 'ScarlettJohansson',\n",
       " 'MorganFreeman',\n",
       " 'WoodyHarrelson',\n",
       " 'ColinFarrell',\n",
       " 'JulianneMoore',\n",
       " 'SteveCarell',\n",
       " 'CharlizeTheron',\n",
       " 'HughJackman',\n",
       " 'AdamSandler',\n",
       " 'CateBlanchett',\n",
       " 'JasonStatham',\n",
       " 'ColinFirth',\n",
       " 'BenAffleck',\n",
       " 'BradPitt',\n",
       " 'TomHanks',\n",
       " 'GerardButler',\n",
       " 'ChristianBale']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeating steps to create a sorted dictionary as above\n",
    "dictionary_act = gensim.corpora.Dictionary(tokenized_actors)\n",
    "new_dict_act=dictionary_act.cfs\n",
    "#Sorting by wordcount\n",
    "new_dict_act_2 = sorted(new_dict_act.items(), key=lambda x:x[1],reverse=True)\n",
    "#Translating word index to word\n",
    "ranked_actors=[]\n",
    "for i in range(len(new_dict_act_2)):\n",
    "    ranked_actors.append(dictionary2[new_dict_act_2[i][0]])\n",
    "ranked_actors.remove('.')\n",
    "#Top 30 most billed actors\n",
    "ranked_actors[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
